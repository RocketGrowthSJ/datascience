{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "> 이해하기 위해 가장 좋은 방법은 구현해보는 것이라고 생각한다. 구현해보면서 배워보자.\n",
    "\n",
    "reference : [how to write a MapReduce Framework in Python](https://medium.com/@nidhog/how-to-quickly-write-a-mapreduce-framework-in-python-821a79fda554)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맵리듀스 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. HDFS 구현하기\n",
    "\n",
    "> 진짜 구현한다는 것은 아니고, 맵리듀스 프레임워크는 기본적으로 HDFS 위에서 돌아가기 때문에, HDFS(haddop distributed file system)과 같이, 작업 파일을 관리해주는 가상의 핸들러가 필요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from operator import itemgetter as operator_ig\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDFSHandler(object):\n",
    "    \"\"\"HDFS Handler class\n",
    "    입력 파일을 Splitting하고 출력 파일을 Joining 해주는 역할\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_file_path, \n",
    "                 output_dir=\"./output_files\", \n",
    "                 temp_dir=\"./temp_map_files\"):\n",
    "        \"\"\"        \n",
    "        :param input_file_path: input file path\n",
    "        :param output_dir: output directory path\n",
    "        \"\"\"\n",
    "        self.input_file_path = input_file_path\n",
    "        self.input_dir = os.path.split(input_file_path)[0]\n",
    "        self.file_name = self.get_input_filename(input_file_path)\n",
    "        self.output_dir = output_dir\n",
    "        self.temp_dir = temp_dir\n",
    "        \n",
    "        # 작업 및 저장 공간 확보\n",
    "        os.makedirs(self.temp_dir, exist_ok=True)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def get_input_filename(self, input_file_path):\n",
    "        \"\"\"return the name of the input file to be split into chunks\n",
    "        \"\"\"\n",
    "        file_name = os.path.split(input_file_path)[1].split('.')[0]\n",
    "        return file_name\n",
    "        \n",
    "    def get_input_split_file(self, index):\n",
    "        \"\"\" return the name of the current split file corresponding to\n",
    "        the given index\n",
    "        \"\"\"\n",
    "        split_file_name = f\"{self.file_name}_{index}.ext\"\n",
    "        return os.path.join(self.input_dir, split_file_name)\n",
    "    \n",
    "    def get_temp_map_file(self, index, reducer):\n",
    "        \"\"\" return the name of the temporary map file \n",
    "        corresponding to the given index\n",
    "        \"\"\"\n",
    "        temp_file_name = f\"map_{self.file_name}_{index}-{reducer}.ext\"\n",
    "        \n",
    "        return os.path.join(self.temp_dir, temp_file_name)\n",
    "    \n",
    "    def get_output_file(self, index):\n",
    "        reduce_file_name = f\"reduce_{self.file_name}_{index}.out\"\n",
    "        return os.path.join(self.output_dir, reduce_file_name)\n",
    "    \n",
    "    def get_output_join_file(self):\n",
    "        \"\"\" return the name of the output file given \n",
    "        its corresponding index\n",
    "        \"\"\"\n",
    "        output_join_name = f\"{self.file_name}.out\"\n",
    "        return os.path.join(self.output_dir, output_join_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFS 파일시스템에서 우리는 크게 3가지 단계로 쪼갰다.\n",
    "\n",
    "* `input_dir` : 작업 입력 파일들이 저장되는 공간\n",
    "* `temp_dir` : mapper의 중간 결과값들이 저장되는 공간\n",
    "* `output_dir` : reducer의 최종 결과값들이 저장되는 공간 \n",
    "\n",
    "기본적으로 input_file_path에서 값을 가져오면 아래의 순서대로 처리된다.\n",
    "\n",
    "example : \n",
    "\n",
    "* input_file_path (ex : `../books/pride_and_predudice.txt`)\n",
    "    > 주어진 파일 path\n",
    "* file_name(ex : `pride_and_prejudice` )\n",
    "     > 작업 처리할 때, 파일 이름의 고유 식별 기준\n",
    "* input_split_file(ex : `pride_and_prejudice_1.ext`) \n",
    "    > mapper thread에 전달할 입력 데이터. 예시로 따지면, 1번째 mapper thread가 처리할 데이터\n",
    "* temp_split_file(ex : `map_pride_and_prejudice_1-2.ext`)\n",
    "    > 1번째 mapper의 결과물로, 2번째 reducer에게 전달할 결과물\n",
    "* output_file_name(ex : `reduce_pride_and_prejudice_1.out`)\n",
    "    > reducer thread의 결과값들. dPtlfh Ekwlaus, 1번째 reducer thread의 결과들\n",
    "* output_join_name(ex : `pride_and_prejudice.out`)\n",
    "    > reducer의 결과들을 최종적으로 join한 값들\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 우리는 이제 mapper에 보내기 위해 input 파일을 쪼개고, reducer의 결과물들을 합쳐주는 메소드가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(self, num_splits):\n",
    "    \"\"\"split a file into multiple files.\n",
    "    \n",
    "    : param num_splits: the number of chunks to split the file into.\n",
    "    \"\"\"\n",
    "    # (1) input 파일 읽어오기\n",
    "    # (2) split할 크기 계산하기\n",
    "    # (3) 순회하며 input 파일을 자르기\n",
    "    # (4) 자른 파일들을 각각 저장하기\n",
    "    raise NotImplementedError\n",
    "\n",
    "HDFSHandler.split_file = split_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구현은 아래와 같은 방식으로 진행하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_file_split(self, split_index, index):\n",
    "    \"\"\"initialize a split file by opening and adding an index.\n",
    "    :param split_index: the split index we are currently on, to be used for naming the file.\n",
    "    :param index: the index given to the file.\n",
    "    \"\"\"\n",
    "    file_split = open(self.get_input_split_file(split_index-1), \"w+\")\n",
    "    file_split.write(str(index) + \"\\n\")\n",
    "    return file_split\n",
    "\n",
    "HDFSHandler.initiate_file_split = initiate_file_split\n",
    "\n",
    "def split_file(self, num_splits):\n",
    "    \"\"\"split a file into multiple files.\n",
    "    \n",
    "    : param num_splits: the number of chunks to split the file into.\n",
    "    \"\"\"\n",
    "    # (1) input 파일 읽어오기\n",
    "    with open(self.input_file_path, \"r\") as file:\n",
    "        file_content = file.read()\n",
    "    # (2) split할 크기 계산하기\n",
    "    file_size = os.path.getsize(self.input_file_path)\n",
    "    split_size = file_size / num_splits + 1\n",
    "    \n",
    "    # (3) 순회하며 input 파일들을 자르기\n",
    "    (index, current_split_index) = (1, 1)\n",
    "    current_split_file = self.initiate_file_split(current_split_index, index)\n",
    "    for character in file_content:\n",
    "        # (4) 자른 파일들을 각각 저장하기\n",
    "        current_split_file.write(character)\n",
    "        if index>split_size*current_split_index+1 and character.isspace():\n",
    "            current_split_file.close()\n",
    "            current_split_index += 1\n",
    "            current_split_file = self.initiate_file_split(current_split_index, index)\n",
    "        index += 1\n",
    "    current_split_file.close()\n",
    "    \n",
    "HDFSHandler.split_file = split_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_files(self, num_files, clean=False,sort=True,decreasing=True):\n",
    "    \"\"\"join all the files in the output directory into a\n",
    "    single output file.\n",
    "\n",
    "    :param num_files: total number of files.\n",
    "    :param clean: if True the reduce outputs will be deleted,\n",
    "    by default takes the value of self.clean.\n",
    "    :param sort: sort the outputs.\n",
    "    :param decreasing: sort by decreasing order, high value\n",
    "    to low value.\n",
    "    :return output_join_list: a list of the outputs\n",
    "    \"\"\"\n",
    "    # (1) reducer의 결과물들을 가져옴\n",
    "    output_join_list = []\n",
    "    for reducer_index in range(num_files):\n",
    "        with open(self.get_output_file(reducer_index), \"r\") as f:\n",
    "            output_join_list += json.load(f)\n",
    "        if clean:\n",
    "            os.unlink(self.get_output_file(reducer_index))\n",
    "    # (2) 결과값들을 정렬\n",
    "    if sort:\n",
    "        output_join_list.sort(key=operator_ig(1), reverse=decreasing)\n",
    "    # (3) 저장\n",
    "    with open(self.get_output_join_file(self.output_dir), \"w+\") as f:\n",
    "        json.dump(output_join_list, f)\n",
    "    return output_join_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 맵리듀스 프레임워크 잡기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapReduce(object):\n",
    "    \"\"\"MapReduce class representing the mapreduce model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_file_path, output_dir, \n",
    "                 n_mappers=4, n_reducers=4, clean=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param input_file_path: input file의 위치\n",
    "        :param output_dir: output file이 있는 디렉토리\n",
    "        :param n_mappers: mapper thread의 갯수 \n",
    "                          -> 원래는 입력 파일 / 스플릿 크기(블록크기) 로 지정\n",
    "        :param n_reducers: reducer thread의 갯수\n",
    "        :param clean: 참이면, 임시 파일들이 삭제\n",
    "        \"\"\"\n",
    "        self.input_file_path = input_file_path\n",
    "        self.output_dir = output_dir\n",
    "        self.n_mappers = n_mappers\n",
    "        self.n_reducers = n_reducers\n",
    "        self.clean = clean\n",
    "        \n",
    "        # 처리할 파일을 읽어오고 저장할 파일 핸들러 가져오기\n",
    "        self.file_handler = HDFSHandler(self.input_file_path, \n",
    "                                        self.output_dir)\n",
    "        self.file_handler.split_file(self.n_mappers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맵 리듀스는 하나의 프로그래밍 모델(디자인 패턴)! 매우 간단한 작업이지만, 그러한 작업이 엄청나게 많이 해야 할 때(example, word counts) 사용할 수 있는 Programming Model입니다. Map Reduce는 큰 묶음의 Job을 작은 단위로 쪼개고(split), 계산하고(map), 합치는(reduce) 것으로 구성된다.\n",
    "\n",
    "이러한 작업은 \"parallerl computing\"이라는 이름으로 예전부터 있었는데, 분산 작업을 직접 구현하기에는 많은 어려움이 있었다.\n",
    "\n",
    "1. 여러 개의 물리적 컴퓨팅 자원이 필요하고\n",
    "2. 그 컴퓨터를 하나로 묶어 제어할 Software를 Install 해야 하며\n",
    "3. 그 과정에서 생기는 수 많은 번거로운 작업들을 해야 하기 \n",
    "\n",
    "때문이다.\n",
    "\n",
    "보통 맵리듀스는 아래의 순서를 따른다.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/2136A84B59381A8428)\n",
    "\n",
    "여기서 크게 2 단계(Map, Reduce)의 단계로 구성된다고 볼 수 있다. 우리는 위의 `__init__`에서 지정된 input_dir에 저장된 파일들을 읽어오고(*원래는 hdfs 포맷을 읽어와야 겠지만, 그거까지 구현은 무리데스네...*), output_dir에 저장하는 식으로 진행된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 맵퍼와 리듀서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 맵리듀스에서의 입력과 출력은 `<key-value>`의 쌍으로 움직인다. 맵퍼와 리듀서 모두 `<key-value>` 를 쌍으로 받아서 처리한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "input_split_file = open(settings.get_input_split_file(index), \"r\")\n",
    "key = input_split_file.readline()\n",
    "value = input_split_file.read()\n",
    "input_split_file.close()\n",
    "if(self.clean):\n",
    "    os.unlink(settings.get_input_split_file(index))\n",
    "mapper_result = self.mapper(key, value)\n",
    "for reducer_index in range(self.n_reducers):\n",
    "    temp_map_file = open(settings.get_temp_map_file(index, reducer_index), \"w+\")\n",
    "    json.dump([(key, value) for (key, value) in mapper_result \n",
    "                                if self.check_position(key, reducer_index)]\n",
    "                , temp_map_file)\n",
    "    temp_map_file.close()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(self, key, value):\n",
    "    \"\"\"key-value를 읽어서 필터링하거나 다른 값으로 변환시켜주는 함수\n",
    "    \n",
    "    -> 실제로 사용자가 구현해야 하는 부분\n",
    "    \"\"\"\n",
    "    #TODO: 여기를 구현해야함\n",
    "    raise NotImplementedError\n",
    "\n",
    "def run_mapper(self, ps_id):\n",
    "    \"\"\" 구현된 mapper 메소드를 실행\n",
    "    \n",
    "    :param ps_id: 실행하고자 하는 process id\n",
    "    \"\"\"\n",
    "    # (1) get the hdfs file\n",
    "    with open(self.file_handler.get_input_split_file(ps_id), \"r\") as file:\n",
    "        key = file.readline()\n",
    "        value = file.read()\n",
    "    # (2) [optional] if clean, remove temp file\n",
    "    if(self.clean):\n",
    "        os.unlink(self.file_handler.get_input_split_file(ps_id))\n",
    "    # (3) get the result of the mapper\n",
    "    mapper_result = self.mapper(key, value)\n",
    "    for reducer_index in range(self.n_reducers):\n",
    "        # (4) store the result to be used by the reducer        \n",
    "        with open(self.file_handler.get_temp_map_file(ps_id, reducer_index), \"w+\") as file:\n",
    "            json.dump([(key, value) for (key, value) in mapper_result \n",
    "                                        if self.check_position(key, reducer_index)]\n",
    "                        , file)\n",
    "\n",
    "MapReduce.mapper = mapper\n",
    "MapReduce.run_mapper = run_mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(self, key, values_list):\n",
    "    \"\"\"맵퍼를 통해 출력된 리스트에 새로운 Key를 기준으로 Aggregation하는 함수\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def run_reducer(self, ps_id):\n",
    "    \"\"\" 구현된 reducer 메소드를 실행\n",
    "    \n",
    "    :param ps_id: 실행하고자 하는 process id\n",
    "    \"\"\"\n",
    "    key_values_map = {}\n",
    "    for mapper_index in range(self.n_mappers):\n",
    "        # load the results of the map\n",
    "        with open(self.file_handler.get_temp_map_file(mapper_index, ps_id), \"r\") as file:\n",
    "            mapper_results = json.load(file)\n",
    "            for (key, value) in mapper_results:\n",
    "                if not(key in key_values_map):\n",
    "                    key_values_map[key] = []\n",
    "                try:\n",
    "                    key_values_map[key].append(value)\n",
    "                except Exception as e:\n",
    "                    print(\"Exception while inserting key: \"+str(e))\n",
    "\n",
    "        if self.clean:\n",
    "            os.unlink(self.get_temp_map_file(mapper_index, ps_id))\n",
    "\n",
    "    # for each key reduce the values\n",
    "    key_value_list = []\n",
    "    for key in key_values_map:\n",
    "        key_value_list.append(self.reducer(key, key_values_map[key]))\n",
    "\n",
    "    # store the results for this reducer \n",
    "    with open(self.file_handler.get_output_file(ps_id), \"w+\") as file:\n",
    "        json.dump(key_value_list, file)\n",
    "\n",
    "    \n",
    "MapReduce.reducer = reducer\n",
    "MapReduce.run_reducer = run_reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 잡 트래커\n",
    "\n",
    "우리가 구현한 Mapper와 Reducer의 process를 만들고 관리하는 메소드가 필요하다. 이것이 잡 트래커. 여기에서는 따로 class혹은 method를 두지 않고 run이라는 이름으로 통칭하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "    \"\"\" Executes the map and reduce operations\n",
    "    \"\"\"\n",
    "    # initialize mappers list\n",
    "    map_workers = []\n",
    "    # initialize reducers list\n",
    "    rdc_workers = []\n",
    "    \n",
    "    # run the map step\n",
    "    for thread_id in range(self.n_mappers):\n",
    "        p = Process(target=self.run_mapper, args=(thread_id,))\n",
    "        p.start()\n",
    "        map_workers.append(p)\n",
    "    [t.join() for t in map_workers]\n",
    "    \n",
    "    # run the reduce step\n",
    "    for thread_id in range(self.n_reducers):\n",
    "        p = Process(target=self.run_reducer, args=(thread_id,))\n",
    "        p.start()\n",
    "        map_workers.append(p)\n",
    "    [t.join() for t in rdc_workers]\n",
    "    \n",
    "MapReduce.run = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_outputs(self, clean=True,sort=True,decreasing=True):\n",
    "    \"\"\"Join all the reduce output files into a single output file.\n",
    "\n",
    "    :param clean: if True the reduce outputs will be deleted, by default takes the value of self.clean\n",
    "    :param sort: sort the outputs\n",
    "    :param decreasing: sort by decreasing order, high value to low value\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return self.file_handler.join_files(self.n_reducers, clean, sort, decreasing)\n",
    "    except:\n",
    "        print(\"Exception occured while joining: maybe the join has been performed already  -- \"+str(e))\n",
    "        return []\n",
    "\n",
    "MapReduce.join_outputs = join_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount\n",
    "\n",
    "> WordCount의 예시!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCount(MapReduce):\n",
    "    def __init__(self, input_dir, output_dir, n_mappers, n_reducers):\n",
    "        MapReduce.__init__(self,  input_dir, output_dir, n_mappers, n_reducers)\n",
    "\n",
    "    def mapper(self, key, value):\n",
    "        \"\"\"Map function for the word count example\n",
    "        Note: Each line needs to be separated into words, and each word\n",
    "        needs to be converted to lower case.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        default_count = 1\n",
    "        # seperate line into words\n",
    "        for word in value.split():\n",
    "            if self.is_valid_word(word):\n",
    "                # lowercase words\n",
    "                results.append((word.lower(), default_count))\n",
    "        return results\n",
    "\n",
    "    def is_valid_word(self, word):\n",
    "        \"\"\"Checks if the word is in the defined character range\n",
    "        :param word: word to check\n",
    "        \"\"\"\n",
    "        return all(64 < ord(character) < 128 for character in word)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        \"\"\"Reduce function implementation for the word count example\n",
    "        Note: Each line needs to be separated into words, and each word\n",
    "        needs to be converted to lower case.\n",
    "        \"\"\"\n",
    "        wordcount = sum(value for value in values)\n",
    "        return key, wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = WordCount(\"./input_files/file.ext\", \"./output_files/\", 4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Process Process-2:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-3:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-7-cb3514fefe8d>\", line 26, in run_mapper\n",
      "    json.dump([(key, value) for (key, value) in mapper_result\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-7-cb3514fefe8d>\", line 27, in <listcomp>\n",
      "    if self.check_position(key, reducer_index)]\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "AttributeError: 'WordCount' object has no attribute 'check_position'\n",
      "Process Process-4:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-cb3514fefe8d>\", line 26, in run_mapper\n",
      "    json.dump([(key, value) for (key, value) in mapper_result\n",
      "  File \"<ipython-input-7-cb3514fefe8d>\", line 26, in run_mapper\n",
      "    json.dump([(key, value) for (key, value) in mapper_result\n",
      "  File \"<ipython-input-7-cb3514fefe8d>\", line 27, in <listcomp>\n",
      "    if self.check_position(key, reducer_index)]\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'WordCount' object has no attribute 'check_position'\n",
      "  File \"<ipython-input-7-cb3514fefe8d>\", line 27, in <listcomp>\n",
      "    if self.check_position(key, reducer_index)]\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "AttributeError: 'WordCount' object has no attribute 'check_position'\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-cb3514fefe8d>\", line 26, in run_mapper\n",
      "    json.dump([(key, value) for (key, value) in mapper_result\n",
      "  File \"<ipython-input-7-cb3514fefe8d>\", line 27, in <listcomp>\n",
      "    if self.check_position(key, reducer_index)]\n",
      "AttributeError: 'WordCount' object has no attribute 'check_position'\n",
      "Process Process-5:\n",
      "Process Process-6:\n",
      "Traceback (most recent call last):\n",
      "Process Process-7:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-8:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-8-5bc35057f99c>\", line 15, in run_reducer\n",
      "    mapper_results = json.load(file)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/json/__init__.py\", line 299, in load\n",
      "    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
      "  File \"<ipython-input-8-5bc35057f99c>\", line 14, in run_reducer\n",
      "    with open(self.file_handler.get_temp_map_file(mapper_index, ps_id), \"r\") as file:\n",
      "  File \"<ipython-input-8-5bc35057f99c>\", line 14, in run_reducer\n",
      "    with open(self.file_handler.get_temp_map_file(mapper_index, ps_id), \"r\") as file:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './temp_map_files/map_file_0-2.ext'\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/json/__init__.py\", line 354, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './temp_map_files/map_file_0-1.ext'\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/json/decoder.py\", line 339, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-8-5bc35057f99c>\", line 14, in run_reducer\n",
      "    with open(self.file_handler.get_temp_map_file(mapper_index, ps_id), \"r\") as file:\n",
      "  File \"/Users/ksj/anaconda3/lib/python3.6/json/decoder.py\", line 357, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './temp_map_files/map_file_0-3.ext'\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "word_count.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
